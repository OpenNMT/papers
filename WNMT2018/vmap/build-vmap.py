from __future__ import print_function

from collections import Counter
import argparse
import gzip
import os
import sys

parser = argparse.ArgumentParser()
parser.add_argument('-pt', '--phrase_table', required=True,
                    help="phrase table")
parser.add_argument('-zg', '--zero_generate_list',
                    help="list of terms generated from 0")
parser.add_argument('-ms', '--max_size', default=3, type=int,
                    help="maximal size of source sequences")
parser.add_argument('-mf', '--min_freq', default=2, type=int,
                    help="minimal frequency of pair")
parser.add_argument('-km', '--keep_meaning', default=20, type=int,
                    help="number of meaning to keep per entry")
parser.add_argument('-tv', '--tgt_vocab',
                    help="save target vocabulary for max coverage calculation")
parser.add_argument('-l', '--limit', type=int,
                    help="limit the number of entries (for dev)")

args = parser.parse_args()

zero_gen = set()
if args.zero_generate_list:
    with open(args.zero_generate_list) as f:
        for line in f:
            line = line.strip()
            zero_gen.add(line)

mapping = dict()
# collect all entries by size

tvocab = set()

if os.path.isfile(args.phrase_table + ".gz"):
    args.phrase_table += ".gz"
if os.path.isfile(args.phrase_table):
    if args.phrase_table.endswith(".gz"):
        f = gzip.open(args.phrase_table, "rt")
    else:
        f = open(args.phrase_table)
else:
    raise RuntimeError('%s is not a file' % args.phrase_table)

count = 0
for line in f:
    line = line.strip()
    entries = line.split(" ||| ")
    source = entries[0].split(" ")
    l = len(source)
    for v in entries[1].split(" "):
        tvocab.add(v)
    freq = entries[4].split(" ")
    if float(freq[2]) >= args.min_freq and l <= args.max_size:
        if l not in mapping:
            mapping[l] =  dict()
        if entries[0] not in mapping[l]:
            mapping[l][entries[0]] = list()
        mapping[l][entries[0]].append((entries[1], freq))
    count += 1
    if args.limit and count > args.limit:
        break

f.close()

if args.tgt_vocab:
    with open(args.tgt_vocab, "w") as fw:
        for v in tvocab:
            fw.write("%s\n" % v)

# sort function
def getFreq(item):
    return float(item[1][2])

# return all the vocabs generated by a sequence using shorter sub-sequences
def get_vocabs(l, src):
    lsrc = src.split(" ")
    assert len(lsrc) == l
    vocabs = set(zero_gen)
    for i in range(l):
        for j in range(i+1, l+1):
            if j-i != l:
                seq = " ".join(lsrc[i:j])
                if j-i != l and seq in mapping[j-i]:
                    for v in mapping[j-i][seq]:
                        vocabs.add(v)
    return vocabs

def keep_meaning(mean, vocabs):
    (tgt, freq) = mean
    ltgt = tgt.split(" ")
    keep = set()
    for m in ltgt:
        if m not in zero_gen:
            freq_mean[m] += 1
        if m not in vocabs:
            keep.add(m)
    return keep

print("\t"+" ".join(zero_gen))

# collect frequent meaning - for checking zero_gen list completion
freq_mean = Counter()

# now go through the entries by size and check if they bring new meanings
for l in range(1, args.max_size+1):
    if l not in mapping:
        break
    del_key = []
    for m in mapping[l]:
        lmean = sorted(mapping[l][m], key=getFreq, reverse=True)
        if len(lmean) > args.keep_meaning:
            lmean = lmean[:args.keep_meaning]
        keep = set()
        vocabs = get_vocabs(l, m)
        for mean in lmean:
            keep = keep.union(keep_meaning(mean, vocabs))
        if len(keep) > 0:
            mapping[l][m] = list(keep)
            print(m+"\t"+" ".join(keep))
        else:
            del_key.append(m)
    for m in del_key:
        del mapping[l][m]

freq_means = freq_mean.most_common(20)

for v in freq_means:
    sys.stderr.write("%s\n" % v[0])
